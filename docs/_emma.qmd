---
title: "Why you should use a script-based workflow for your survey"

author: McCrea Cobb | Emma Schillerstrom | Jonah Withers
format: 
  revealjs:
    theme: scss/custom-dark.scss
    code-overflow: wrap
    logo: images/FWS-logo.png
    footer: "Alaska Data Week"
    chalkboard:
      theme: whiteboard
      boardmarker-width: 5
      buttons: false
    embed-resources: false
    code-block-height: 500px
filters:
  - output-line-highlight.lua
editor: source
highlight-style: a11y
---

```{r setup}
#| echo: false

knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      fig.retina = 3, 
                      fig.align = "center")
```

::: columns
::: {.column width="50%"}
Left column
:::

::: {.column width="50%"}
Right column
:::
:::

## Blank Slide

Content

::: notes
:::

# Data Clean-Up

![](images/witch_data_cleanup.png){fig-align="center"}

## Why Clean Up Data in R? {.incremental .smaller}

::: columns
::: {.column width="50%"}
#### [Limitations of]{.cursive} {.center}

#### [Manual Workflow]{.cursive .extrabold .yellow} {.center}

::: incremental
-   Messy

-   Prone to human error

-   Difficult to correct or reverse changes

-   Inefficient

-   Not reusable
:::
:::

::: {.column width="50%"}
#### [Advantages of]{.cursive} {.center}

#### [Scripted Workflow]{.cursive .extrabold .yellow} {.center}

::: incremental
-   Can iterate through large datasets

-   Can manipulate data without overwriting it

-   Can easily undo or reapply changes

-   Changes are documented

-   Avoids repeating steps

-   Allows for automated workflows/pipelines

-   Eliminates future QC workload
:::
:::
:::

::: notes
-   efficient, reusable, not prone to error, and on top of that...

-   working outside of original file

-   can apply change to hundreds of thousands of rows of data in a single step - without ever even having to scroll through the rows

-   great for apps or lab data that comes in a set, untidy format

-   Requires medium initial effort for a future of no effort

-   can apply changes to all rows, all sheets, all files at once
:::

# 

::: scary
Witch Survey
:::

::: {.cursive .center .larger}
Intro to the Data
:::

::: notes
what data are we cleaning up? let me introduce you to the tetlin witch project
:::

## Tetlin Witch Survey {.smaller}

::: columns
::: {.column width="50%"}
\[Insert Map\]
:::

::: {.column width="50%"}
<br>

[Type:]{.extrabold .orange} Annual occurrence survey

[Logistics:]{.extrabold .orange}

-   Completed over 8 days in August

-   Detection (0, 1) recorded at 100 sites/day

-   Conducted for 20 years at Tetlin NWR

[Goal:]{.extrabold .orange} Estimate probability of witch occurrence across the refuge
:::
:::

::: notes
the Tetlin Witch project, not the same as the Blair witch project

lots of biotechs - obviously national funding for witch conservation
:::

## Witch Background

::: columns
::: {.column .center width="50%"}
Do NOT like water

![](images/witch_melting.png){fig-align="center"}
:::

::: {.column .center width="50%"}
DO favor forest

![](images/witch_broom.png){fig-align="center"}
:::
:::

::: notes
Can use to characterize and predict their distribution

Species Background - North American Witch

Began inhabiting Tetlin NWR during the early modern period when widespread witch trials shifted their range northward

Started monitoring them due to concerns surrounding the threat of deforestation, impacting their ability to construct broomsticks
:::

## Where We Are

::: columns
::: {.column width="50%"}
**Completed:**

-   Data collection ✓

-   Data entry ✓

**Next:**

-   Data clean-up ☐
:::

::: {.column width="50%"}
::: {layout="[[-1], [1], [-1]]"}
![Excel workbook with all years of data](images/workbook_sc.png){fig-align="center"}
:::
:::
:::

::: notes
I am the survey coordinator, but this is my first year on the survey. For the last 20 years, the data has been stored in an excel workbook with each year's data on a separate sheet. I inherited a bit of a messy dataset, but I can work with it. No automated data processing workflow exists for this dataset, so let's create one!
:::

## Photos from the Field

{{< fa camera >}} Coming soon to your Wild Weekly...

::: columns
::: {.column width="33%" layout="[[-1], [1], [-1]]"}
![](images/witch_hiding.png){fig-align="center"}
:::

::: {.column width="33%" layout="[[-1], [1], [-1]]"}
![](images/witch_moose.png){fig-align="center"}
:::

::: {.column width="33%" layout="[[-1], [1], [-1]]"}
![](images/witch_tetlin.png){fig-align="center"}
:::
:::

::: notes
none
:::

# 

::: scary
Witch Survey
:::

::: {.cursive .center .larger}
Starting Our Script
:::

::: notes
:::

## Getting Set Up

<br>

✓ Created a new R project

✓ Set up a standard file directory structure

<br>

::: center
[Time to get coding!]{.large .yellow}
:::

::: notes
following Jonah's guidance... open a new blank R script file and...
:::

## Starting a New Script

-   Install packages

    ```{r}
    #| echo: true
    #| eval: false
    install.packages(c("tidyverse","readxl","janitor"))
    ```

    -   For non-CRAN packages, use `devtools::install_github("author/package_name")`

-   Use `library()` to load

    -   Sits at top of script and updated as you go

    ```{r}
    #| echo: true
    #Load packages
    library(tidyverse)
    library(readxl)
    library(janitor)
    ```

::: notes
To use external functions, we need to load their packages

these packages are all in the CRAN (comprehensive R archive network) repository

you can also install non-CRAN packages, such as those developed by your regional biometrician, directly from GitHub using a package called devtools

once we... can go ahead and load them. only have to install once (so we can remove the installation code), but need to reload in every script

sits at top of script -

tells user what packages they need to have installed
:::

## Starting a New Script

This is called a [*comment*]{.yellow}.

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "1"
#Load packages
library(tidyverse)
library(readxl)
library(janitor)
```

A comment precedes [machine-readable language]{.yellow} and supplies a translation to [human-readable language]{.yellow}.

::: center
[Human Readability]{.orange .extrabold} {{< fa handshake >}} [Reusability]{.orange .extrabold}
:::

::: callout-tip
## Best Practice

Document every line to few lines of code with a comment.
:::

::: notes
none
:::

## Starting a New Script

Basic documentation is as easy as a "`#`" (shortcut: `CTRL-SHIFT-C`)

```{r}
#| echo: true
#| eval: false
# Any code or text that is commented will not be executed,
but if it is not commented, R will treat it as code,
so this will return an ERROR.
```

::: callout-tip
## Tip!

The `#` can also be used to temporarily disable lines of code for testing purposes.
:::

::: notes
It serves as a reminder for what your code accomplishes and why it was included.

Translating coding language to human language is critical for yourself AND others.
:::

# Data Clean-Up: [Importing]{.orange}

![](images/witch_import.jpg){fig-align="center"}

## Importing Data

Can read data from a...

1.  [Locally stored file]{.orange} (*.csv, .xlsx, etc.*)

::: small
*Example:*
:::

```{r}
#| echo: true
#| eval: false
my_data <- read.csv("path/to/filename.csv")
```

2.  [Online source]{.orange} (*download link, API, ServCat, etc.*)

::: small
*Examples:*
:::

```{r}
#| echo: true
#| eval: false
my_data <- read.csv(download.file("www.url.com","save_as_filename.csv"))
```

```{r}
#| echo: true
#| eval: false
my_data <- GET(url = "api-url.com/input-id", body = list("parameter1","parameter2"))
```

::: notes
Built-in and installable functions exist for reading data from various sources Like a link to the online authoritative source for a dataset
:::

## Importing Data

Extra Step: merge multiple tables

```{r}
#| echo: true
#| eval: false
#Join tables by common variables
all_data <- left_join(observation_data, site_data)
```

::: callout-note
## Suggestion

You can often replace complex MS Access databases with R scripts. Using R to join related tables also ensures your files are all in an open format.
:::

::: notes
none
:::

# 

::: scary
Witch Survey
:::

::: {.cursive .center .larger}
Importing Our Data
:::

::: notes
:::

## Importing Our Data

Use `read_excel()` to import data from a specific sheet in a workbook

-   Set new "dataframe" as a [*variable*]{.yellow}

```{r}
#| echo: true
#Import raw data
witch_data <- read_excel("data/witch_survey.xlsx", sheet = "2024")
```

::: callout-note
## Note

Variables are the basis of reusability!
:::

::: notes
Variables allow us to easily reference complex expressions and enable reusability. If I write all my code referencing the "witch_data" variable and I decide I want to change the input file, I can easily do that in this one line and not have to tamper with any of the rest of my code.
:::

# Data Clean-Up: [Tidying]{.orange}

![](images/witch_tidying.jpg){fig-align="center"}

## Tidy Data

::: {layout="[[-1], [1], [-1]]"}
![](images/tidy_graphic.png){fig-align="center"}
:::

::: notes
none
:::

## Why Tidy Data?

::: columns
::: {.column .incremental width="50%"}
<br>

-   Standardization

-   Interpretability

-   Machine readability

-   Ease of use and reuse

-   Conducive to metadata
:::

::: {.column width="50%"}
::: {layout="[[-1], [1], [-1]]"}
![](images/why_tidy_data.jpg){fig-align="center"}
:::
:::
:::

::: notes
By keeping data in a standardized format with clear attributes and values, the data can be easily interpreted by humans and manipulated through computers
:::

# 

::: scary
Witch Survey
:::

::: {.cursive .center .larger}
Tidying Our Data
:::

::: notes
:::

## Tidying Our Data

Let's take a peak at what we are working with:

```{r}
#| echo: false
#| eval: true
library(gt)
gt(head(witch_data, 6)) %>% opt_stylize(style = 6, color = 'cyan')
#%>% tab_caption("First 6 lines of dataset")
```

::: center
[*First 6 lines of dataset*]{.smaller}
:::

::: callout-caution
## Why is our data not tidy?

**Answer:** Some column names are values NOT variables
:::

::: notes
-   This means we need to restructure our data
:::

## Tidying Our Data

Let's use `tidyverse` to tidy our data in one line by "pivoting"

```{r}
#| echo: true
#Tidy data structure
tidy_data <- pivot_longer(witch_data, cols = starts_with("08"), names_to = "Date", values_to = "Presence", names_transform = list(Date = mdy), values_transform = list(Presence = as.character))
```

::: small
Updated dataframe:
:::

::: smaller
*Row Count: 800*
:::

::: {style="height:320px; overflow-y: scroll"}
```{r}
#| echo: false
gt(tidy_data) %>% opt_stylize(style = 6, color = 'cyan')
```
:::

::: notes
none
:::

## Tidying Our Data {.scrollable}

::: small
-   In tidy data, column names are variables, so they should be structured as such

-   Common variable naming conventions:

    -   `camelCase`
    -   `snake_case`

-   We can use the `janitor` package to fix all our column names in a single line of code
:::

```{r}
#| echo: true
#Clean column names
tidy_data <- clean_names(tidy_data)
```

```{r}
#| echo: false
gt(head(tidy_data,1)) %>% opt_stylize(style = 6, color = 'cyan')
```

::: {.smaller .center}
*New column headers*
:::

::: notes
none
:::

# Data Clean-Up: [Exploratory Data Analysis (EDA)]{.orange}

![](images/witch_eda.jpg){fig-align="center"}

## What is EDA? {.small}

Exploratory Data Analysis (EDA) = [getting to know your data before drawing conclusions]{.orange}, often through summarization and/or visualization

*Example packages:* `skimr`, `corrplot`, `summary_tools`, `DataExplorer`, `assertr`

<br>

::: {.cursive .small}
[Are there errors in my data?]{.fragment} [Are there outliers?]{.fragment} [How variable is my data?]{.fragment}

[Are my data within their expected range of values?]{.fragment}

[Are my variables correlated?]{.fragment} [Do my variables follow their expected distributions?]{.fragment}

[What hypotheses can I generate?]{.fragment} [Are the assumptions for my analyses met?]{.fragment}
:::

::: notes
Lends itself to quality control
:::

# 

::: scary
Witch Survey
:::

::: {.cursive .center}
Exploring Our Data & Performing Quality Control
:::

::: notes
:::

## Exploring Our Data (EDA)

Let's [rewind {{< fa backward-fast >}}]{.orange} and take a closer look at our starting data

-   First, with `str()`

    ```{r}
    #| echo: true
    #| eval: true
    #| class-output: highlight
    #| output-line-numbers: "|5|9|11"
    #Glimpse of dataset,including datatypes of columns
    str(witch_data)
    ```

::: notes
-   (Restructing our data requires a little bit more knowledge on the state of the data because we are going to have to combine columns, and columns of different types cannot be combined)
-   using some R built-in functions
:::

## Exploring Our Data (EDA) {.scrollable chalkboard-buttons="true"}

-   Then, with `summary()`

```{r}
#| echo: true
#| eval: true
#| class-output: highlight
#| output-line-numbers: "|3|10|17|14"
#Summary of dataset, including summary stats
summary(witch_data)
```

::: notes
let me see what might be the cause
:::

##  {.small}

#### Frank and Stein the Wildlife Biologists

[*Roles: field work, data recording*]{.small}

![](images/frank_stein_graphic.png){fig-align="left"}

#### Casper the friendly Biotech

[*Role: data entry/digitization*]{.small}

![](images/casper_graphic.png){fig-align="left"}

![](images/qc_6.jpg){.absolute top="360" right="280" width="100" height="100"} ![](images/qc_2.jpg){.absolute top="360" right="170" width="100" height="100"} ![](images/qc_7.jpg){.absolute top="360" right="60" width="100" height="100"}

::: notes
none
:::

## EDA for Quality Control

See why our presence data is non-numeric

```{r}
#| echo: true
#| warning: false
#Find non-numerics
tidy_data$presence[which(is.na(as.numeric(tidy_data$presence)))]
```

Replace all instances of `"none"` with `0`

```{r}
#| echo: true
#Fix non-numerics
tidy_data$presence[which(tidy_data$presence == "none")] <- 0
tidy_data$presence <- as.integer(tidy_data$presence)
```

::: notes
and convert the column back to a numeric type
:::

## EDA for Quality Control {.scrollable}

Explore distribution

```{r}
#| echo: true
#| eval: true
#Plot histogram of "presence" values
hist(tidy_data$presence)
```

Correct misread numbers

```{r}
#| echo: true
#Fix typos
tidy_data$presence[which(tidy_data$presence == 6)] <- 0
tidy_data$presence[which((tidy_data$presence == 2) | (tidy_data$presence == 7))] <- 1
```

::: notes
applies change to all 800 lines of data in one line of code
:::

## More EDA Options {.small}

::: columns
::: {.column width="50%"}
-   Use `DataExplorer` to generate a comprehensive report
    -   Includes summary statistics, distributions, missing data, PCA, qq plots, and more

```{r}
#| echo: true
#| eval: false
create_report(tidy_data, output_format = pdf(), output_file = "report.pdf")
```

-   Endless options!

```{r}
#| eval: true
library(DataExplorer)
```
:::

::: {.column width="50%"}
{{< pdf report.pdf width=100% height=500 >}}
:::
:::

::: notes
not necessary for our simple dataset but a great, customizable option
:::

# Preservation

![But don't do it this way\...](images/witch_preserve.jpg){fig-align="center"}

## Preserving Data and Data Assets {.small}

Yes, **already**!

Preservation should occur [several times]{.orange} during the data management lifecycle and include all data, metadata, and data assets (e.g., protocols, presentations, reports, code)

::: callout-tip
## Tip

Contact your program's data manager for program-specific preservation guidelines.

-   Fisheries and Ecological Services - *Jonah Withers*

-   Migratory Birds Management - *Tammy Patterson*

-   National Wildlife Refuge Program - *Caylen Cummins*

-   Other programs - *Hilmar Maier*
:::

[Reminder:]{.yellow} Reproducibility and reuse are impossible without *findability*

::: notes
-   In the FWS, we must preserve our data and data assets in a Board approved repository, which is ServCat or ScienceBase
:::

# 

::: scary
Witch Survey
:::

::: {.cursive .center}
Preserving Our Data
:::

::: notes
:::

## Preserving Our Data

[To Do List {{< fa pencil >}}:]{.extrabold .yellow}

1.  Write machine-readable metadata, including a data dictionary
2.  Preserve raw data and script file (if reusing) in ServCat
3.  Link new ServCat references to the "Tetlin Witch Survey" ServCat Project

::: fragment
A step further...

::: center
Tidy, QC, compile, and preserve all 20 years of data
:::
:::

::: notes
-   Write machine-readable metadata for the tidied data file, including a data dictionary defining each of the variables (columns) and their domains
-   Preserve data, script files, and reports
    -   In ServCat, create separate reference for each and link all to the project reference corresponding to our witch survey
-   Feeling ambitious...
:::

## Preserve Data Assets

Uh Oh {{< fa face-frown >}}...

-   All past workbook sheets are in the same untidy format

-   And Frank used `none` instead of `0` since the survey began

::: fragment
Don't worry, [we can reuse our script]{.yellow}!

::: callout-note
## Suggestion

Wrapping the script into a function makes this even more streamlined (*optional*)
:::
:::

::: notes
-   Would be a nightmare to manually fix, but we can reuse the script we just created
:::

## Preserve Data Assets {.scrollable}

::: panel-tabset
### [Basic: Script]{.smaller}

Can reuse and rerun script for other years simply by replacing sheet name in "Import Data" step

```{r}
#| echo: true
#| eval: false

#Load packages
library(tidyverse)
library(readxl)
library(janitor)

#Import raw data
witch_data <- read_excel("data/witch_survey.xlsx", sheet = "2024")

#Tidy data structure
tidy_data <- pivot_longer(witch_data, cols = starts_with("08"), names_to = "Date", values_to = "Presence", names_transform = list(Date = mdy), values_transform = list(Presence = as.character))

#Clean column names
tidy_data <- clean_names(tidy_data)

#Fix non-numerics
tidy_data$presence[which(tidy_data$presence == "none")] <- 0
tidy_data$presence <- as.integer(tidy_data$presence)

#Fix typos
tidy_data$presence[which(tidy_data$presence == 6)] <- 0
tidy_data$presence[which((tidy_data$presence == 2) | (tidy_data$presence == 7))] <- 1

#Export tidy csv
write.csv(tidy_data, file = "tidy_witch_data.csv", row.names = FALSE)
```

### [Advanced: Function]{.smaller}

OR wrap script into function

```{r}
#| echo: true
#| eval: false

clean_witch_data <- function(year){
  #Import raw data
  observation_data <- read_excel("witch_survey.xlsx", sheet = paste0(year))
  
  #Tidy data structure
  tidy_data <- pivot_longer(witch_data, cols = starts_with("08"), names_to = "Date", values_to = "Presence", names_transform = list(Date = mdy), values_transform = list(Presence = as.character))
  
  #Clean column names
  tidy_data <- clean_names(tidy_data)
  
  #Fix non-numerics
  tidy_data$presence[which(tidy_data$presence == "none")] <- 0
  tidy_data$presence <- as.integer(tidy_data$presence)
  
  #Fix typos
  tidy_data$presence[which(tidy_data$presence == 6)] <- 0
  tidy_data$presence[which((tidy_data$presence == 2) | (tidy_data$presence == 7))] <- 1
  
  #Return tidy dataframe
  return(tidy_data)
}
```

### [Advanced: Documented Function]{.smaller}

Make sure to add proper documentation!

```{r}
#| echo: true
#| eval: false

#' Tidy and QC witch data
#'
#' @param year The year (YYYY) representing the worksheet name to pull data from
#'
#' @return A tidied dataframe
#' @export
#'
#' @examples
#' clean_witch_data(year = 2024)
clean_witch_data <- function(year){
  #Import raw data
  observation_data <- read_excel("witch_survey.xlsx", sheet = paste0(year))
  site_data <- read_excel("witch_survey.xlsx", sheet = "Site Info")
  
  #Relate tables: observation data and site data
  witch_data <- left_join(observation_data, site_data)
  
  #Tidy data structure
  tidy_data <- pivot_longer(witch_data, cols = starts_with("08"), names_to = "Date", values_to = "Presence", names_transform = list(Date = mdy), values_transform = list(Presence = as.character))
  
  #Clean column names
  tidy_data <- clean_names(tidy_data)
  
  #Fix non-numerics
  tidy_data$presence[which(tidy_data$presence == "none")] <- 0
  tidy_data$presence <- as.integer(tidy_data$presence)
  
  #Fix typos
  tidy_data$presence[which(tidy_data$presence == 6)] <- 0
  tidy_data$presence[which((tidy_data$presence == 2) | (tidy_data$presence == 7))] <- 1
  
  #Return tidy dataframe
  return(tidy_data)
}
```
:::

::: notes
none
:::

## Preserve Data Assets

Can run our function for each year and compile data as follows:

```{r}
#| echo: true
#| eval: false

## Clean and compile all years of data
#Tidy first year of data
compiled_data <- clean_witch_data(2004)

#Iterate through each year, tidy data for that year, and join with rest of tidy data
for(val in 2005:2024){
  compiled_data <- full_join(compiled_data, clean_witch_data(year = val))
}

#Export
write.csv(compiled_data, file = "compiled_witch_data.csv", row.names = FALSE)
```

::: notes
none
:::

## Preserve Data Assets

Tidy, compiled data ready to be preserved and shared

[*Rows: 16,000*]{.smaller}

::: smaller
::: {style="height:520px"}
```{r}
#| eval: TRUE
library(DT)
DT::datatable(tidy_data, options = list(
  pageLength = 9,
  dom = 't',
  columnDefs = list(
      list(targets = "_all", className = "custom-header")
    )
  )
)
```
:::
:::

::: notes
none
:::

## Preserve Data Assets

Eliminate input files through ServCat API requests!

```{r}
#| eval: false
#| echo: true
library(httr)

#' Pull complete Tetlin witch survey dataset from ServCat
#'
#' @return dataframe
#' @export
#'
#' @examples
#' pull_witch_data()
pull_witch_data <- function(){
  #Request
  url <- "https://ecos.fws.gov/ServCatServices/servcat-secure/v4/rest/DownloadFile/1234"
  response <- GET(url = url, config = authenticate(":",":","ntlm"), add_headers("Content-Type" = "application/json"), verbose())
  
  #Extract file name from response header
  header <- headers(response)
  filename <- sub('.*filename="([^"]+)".*',"\\1", header$`content-disposition`)
  
  #Save file
  writeBin(content(response,"raw"), paste0("data/",filename))
  
  #Import dataset
  return(read.csv(paste0("data/",filename)))
}

```

::: notes
none
:::

# Break Time

## Reuse

Like magic {{\< fa wand-sparkles \>}

-   Can now repeat all tidying, analyses, and visualizations next year with just the click of a button!
-   Medium to high initial investment for dramatically reduced future workload

::: notes
none
:::

## Resources

-   Available in the README.md
-   If you are in Refuges and interested in developing an automated workflow for your survey but need some assistance, submit an I&M Tech Request

::: notes
none
:::
