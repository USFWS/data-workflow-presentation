---
format: 
  revealjs:
    theme: scss/custom-dark.scss
    logo: images/FWS-logo.png
    footer: "[Alaska Data Week 2024](https://doimspp.sharepoint.com/sites/fws-FF07S00000-data/SitePages/Alaska-Data-Week-2024.aspx){.author}"
    mermaid: 
      theme: neutral
editor: source
highlight-style: tango
---

```{r setup_index}
#| echo: false

options(repos = c(CRAN = "https://cloud.r-project.org/"))

knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      fig.retina = 3, 
                      fig.align = "center")
```

# A Reproducible Data Workflow {.smaller}

<br><br>

::: columns
::: {.column width="33%"}
[**McCrea Cobb**]{.author}

::: smaller
Refuge Inventory and Monitoring\
{{< fa envelope title="Email address logo" >}} [mccrea_cobb\@fws.gov](mailto:mccrea_cobb@fws.gov)\
{{< fa brands github title="The GitHub octocat logo" >}} [mccrea-cobb](https://github.com/mccrea-cobb)
:::
:::

::: {.column width="34%"}
[**Emma Schillerstrom**]{.author}

::: smaller
Refuge Inventory and Monitoring\
{{< fa envelope title="Email address logo" >}} [emma_shillerstrom\@fws.gov](mailto:emma_schillerstrom@fws.gov)\
{{< fa brands github title="The GitHub octocat logo" >}} [eschillerstrom-usfws](https://github.com/eschillerstrom-usfws)
:::
:::

::: {.column width="33%"}
[**Jonah Withers**]{.author}

::: smaller
Fisheries and Ecological Services\
{{< fa envelope title="Email address logo" >}} [jonah_withers\@fws.gov](mailto:jonah_withers@fws.gov)\
{{< fa brands github title="The GitHub octocat logo" >}} [JonahWithers-USFWS](https://github.com/JonahWithers-USFWS)
:::
:::
:::

<br><br>

::: {.center .smaller}
{{< fa brands github title="The GitHub octocat logo" >}} <https://github.com/USFWS/data-workflow-presentation>
:::

::: notes
Welcome everyone. My name is Jonah Withers and I'm a data manager with the Fisheries and Ecological Services program. My colleagues and co-presenters of today's talk are McCrea Cobb and Emma Schillerstrom. McCrea is a biometrician with the refuges program and Emma is a data management technician with the refuges program.
:::

## Overview

<br>

-   What is a script-based workflow?
-   Why bother to learn R and follow a script-based workflow?
-   Our recommended workflow

::: notes
In this presentation, we're going to cover what script-based workflows are, why they're worth using, and some best practices to follow when you're using them. Our hope is that by the end of this presentation, you'll understand the advantages of using a script-based data workflow over a manual data workflow, you'll be familiar with some best-practices and understand why they're important, and have a better understanding of resources and staff available to assist you with developing code-based workflows.
:::

## What is a script-based workflow?

<br>

<br>

A script-based workflow is a structured process that utilizes code to automate tasks

::: notes
So what is a script-based workflow? A script-based workflow is a structured process that utilizes code to automate tasks. This means we use code rather than manual processes to process our data.
:::

## Manual workflow

![](images/traditional_workflow_1.png){.shadow width="80%" fig-align="center" fig-alt="An image of a box labeled Field Data Collection with a datasheet icon and hand-held GPS icon"}

::: notes
So for example, in a traditional workflow we may be conducting a survey where we record observations of a given species on a datasheet and say a hand-held GPS unit.
:::

## Manual workflow

![](images/traditional_workflow_2.png){.shadow width="80%" fig-align="center" fig-alt="An image of a box labeled Field Data Collection with a datasheet icon and hand-held GPS icon with an arrow pointing from this box to another box labeled Digital Storage that contains images of microsoft access and excel icons."}

::: notes
Then when we return from the field, we may transcribe those observations into an excel workbook or access database.
:::

## Manual workflow

![](images/traditional_workflow_3.png){.shadow width="80%" fig-align="center" fig-alt="An image of a box labeled Field Data Collection with a datasheet icon and hand-held GPS icon with an arrow pointing from this box to another box labeled Digital Storage that contains images of microsoft access and excel icons with an arrow pointing from this box to another box labeled Quality Control that contains images of microsoft access and excel icons and datasheets."}

::: notes
From there we may do some manual quality control by comparing datasheets to the digital copy and visually reviewing data to find potential errors.
:::

## Manual workflow

![](images/traditional_workflow_4.png){.shadow width="80%" fig-align="center" fig-alt="An image of a box labeled Field Data Collection with a datasheet icon and hand-held GPS icon with an arrow pointing from this box to another box labeled Digital Storage that contains images of microsoft access and excel icons with an arrow pointing from this box to another box labeled Quality Control that contains images of microsoft access and excel icons and datasheets with an arrow pointing to another box labeled Analyses and Mapping with icons of R and ArcPro."}

::: notes
After which we may import the data into something like ArcGIS Pro to create some maps and do spatial analyses and import the data into statistical software like R to create figures and run analyses.
:::

## Manual workflow

![](images/traditional_workflow_5.png){.shadow width="80%" fig-align="center" fig-alt="An image of a box labeled Field Data Collection with a datasheet icon and hand-held GPS icon with an arrow pointing from this box to another box labeled Digital Storage that contains images of microsoft access and excel icons with an arrow pointing from this box to another box labeled Quality Control that contains images of microsoft access and excel icons and datasheets with an arrow pointing to another box labeled Analyses and Mapping with icons of R and ArcPro with an arrow pointing to another box labeled Reports and Presentations containing an image of a microsoft word icon and power point icon."}

::: notes
Next we write a report in Microsoft Word and maybe make a presentation in Microsoft powerpoint.
:::

## Manual workflow

![](images/traditional_workflow_6.png){.shadow width="80%" fig-align="center" fig-alt="An image of a box labeled Field Data Collection with a datasheet icon and hand-held GPS icon with an arrow pointing from this box to another box labeled Digital Storage that contains images of microsoft access and excel icons with an arrow pointing from this box to another box labeled Quality Control that contains images of microsoft access and excel icons and datasheets with an arrow pointing to another box labeled Analyses and Mapping with icons of R and ArcPro with an arrow pointing to another box labeled Reports and Presentations containing an image of a microsoft word icon and power point icon with another arrow pointing to a box labeled Publication and Archiving with a sciencebase and servcat icon."}

::: notes
And finally we archive our products in an approved repository.
:::

## Script-based workflow

![](images/script_based_workflow.png){.shadow width="80%" fig-align="center" fig-alt="An image of a series of boxes labeled Field Data Collection, Digital Storage, Quality Control, Analyses and Mapping, Reports and Presentations, and Publication and Archiving. Each box contains the R icon with the exception of the Field Data Collection box, which has an icon of a tablet and the ESRI Survey123 and Field Maps icons, and the Publication and Archiving box which has the sciencebase and servcat icons."}

::: notes
However, with a script-based workflow all these steps - from acquiring data through publishing reports and presentations - can all be done using code. Now, I want to take a moment here to explicitly point out that there's nothing wrong with using a traditional manual approach but there are some advantages to using a script based-workflow.
:::

## Why use script-based workflow?

![](images/advantages_of_script_based_workflow.png){.shadow width="80%" fig-align="center" fig-alt="A witch-themed image with six scrolls. Each scroll is either labeled documented, reproducible, reduce errors, easily scaled, version control, or sharing."}

::: notes
By using scripts we create a form of documentation that lays out the steps we use to manage and process our data and documents. By documenting our steps in scripts, our workflow becomes reproducible and more transparent to our future selves and others. Scripts also automate our processes which means we reduce our likelihood of introducing errors. This also means our workflow is scalable so that whether we're processing a data set with 10 rows or one with 10 million rows of data our script can be used handle the data in the same way and processing can happen with the click of a button. Version control can also be enabled to track and save changes to your scripts over time. By enabling version control, its easy to share your scripts with others and allow collaborators to contribute to them.
:::

## Why choose {{< fa brands "r-project" size=1x title="R project icon" >}}? 

::: columns

::: {.column width="50%"}

<br>

-   Open-source and free
-   Flexible
-   Widely used in ecology
-   Statistically powerful
-   Active community & tons of packages (`r prettyNum(round((nrow(available.packages()) / 1000) * 1000), big.mark = ",", scientific = FALSE)`!)
:::

::: {.column width="50%"}

<br>

![](images/R_witch-themed_benefits.png){.shadow width="80%" fig-align="center" fig-alt="A witch-themed image denoting the power of R and why to use it."}

:::


:::

::: notes
So why start with R over other coding languages? R is open-source and free. Its extremely flexible, providing options for manipulation, analysis, and visualization among other things. R is also widely used in ecology, statistically powerful, and has a very active community that has developed `r prettyNum(round((nrow(available.packages()) / 1000) * 1000), big.mark = ",", scientific = FALSE)` packages for you to use.
:::

## Create a RStudio project
:::{.center}

<br> 

 {{< fa "file" size=5x title="file icon" >}} {{< fa "arrow-right" size=5x title="arrow icon" >}} {{< fa brands "r-project" size=5x title="R project icon" >}} 
:::

::: notes
So now I'm going to transition into the part of the presentation where we step through our recommended best practices.

To start, we encourage you to create an Rstudio project. What is a R project? You can think of R projects as a project folder that bundles all of your project's files together; very similar to a map-package in ArcGIS. By creating a R project, we establish a default working directory, or root folder for the project. This minimizes file path issues when we go to share the project with others. Additionally, by creating a R project, we give ourselves the options of sharing our code with our collaborators and enabling version control which we'll talk about shortly.
:::

## Create R project

::: columns

::: {.column width="50%"}

<br>

Manually create project

![](images/r_project.png){.shadow width="80%" fig-align="left" fig-alt="Image of Rstudio project setup window."}

:::

::: {.column width="50%"}

<br>

Use code to create project 
```{r}
#| echo: true
#| eval: false

library(refugetools)

create.dir(proj.name = "myProject", 
           dir.name = "C:/Users/jwithers/Presentations")
```

```{r}
#| echo: true
#| eval: false

library(makeProject)

makeProject(name = "myProject", 
            path = "C:/Users/jwithers/Presentations", 
            force = FALSE, 
            author = "Your Name", 
            email = "yourfault@somewhere.net")
```

:::

:::


::: notes
There are several ways to set up an R project. You can do it manually through the RStudio IDE or programmatically using code. Here are two examples of code that use the "makeProject" package and the Refuges I&M team's "refugetools" package.
:::




## Use standardized file structure

::: columns


File structure guidance ^[[Alaska Region Interim Data Management User Guide](https://ak-region-dst.gitbook.io/alaska-region-interim-data-management-user-guide/alaska-data-management-101/file-organization-and-best-practices), [Alaska Data Week 2024 presentations](https://doimspp.sharepoint.com/sites/fws-FF07S00000-data/SitePages/Alaska-Data-Week-2024.aspx), or reach out to a [data manager](https://doimspp.sharepoint.com/sites/fws-FF07S00000-data/SitePages/Contacts.aspx)]

::: {.column width="50%" .smaller}

  - Short-descriptive names
  - Avoid special characters and spaces
  - Name and group files consistently using subfolders
  - Include README

:::

::: {.column width="50%"}
![](images/rdr_file_structure.png){.shadow width="80%" fig-align="right" fig-alt="Image of Alaska Regional Data Repository File Structure"}

:::

:::

::: aside 
For additional information check out:
:::

::: notes
-   After creating a R project, we will want to set up a logical, standardized file structure along with some documentation; such as a READMe file. Why? Standardized file structures help us keep our files organized and make it easier for us, our future selves, and others to navigate, find, and understand what and where your files. We have several resources available on how to establish a good file structure but some main take-aways are to use short-descriptive names, avoid using special characters or spaces in file or folder names, consistently naming and grouping files (remember subfolders are your friend), and documenting the who, what, when, where, and why in a readme .txt document.
:::

## Use standardized file structure

:::notes
There are a variety of folder structures you can use such as A few examples include the Alaska Regional Data Repository template, the refuges template, and the Roxygen2 template. Interim Data Management User Guide (linked below) gives .

but ultimately creating one that is well documented and makes logical sense for you and your audience is best practice.

In addition to creating a standardized file structure, you should always make sure your folder structure is well documented. Emma will go into more details about how documentation but brief ReadMe file saved as a .txt that explains the file structure and the who, what, when, where, why of the project is a good place to start. (Encourage emma to point to Jonah's presentation for additional details)
:::

## Enable version control

<br>

- Manage and track changes to code
- Easily share and collaborate on code
- Provides additional documentation

::: notes
After we've created a R project and set up the project folder structure, we set up version control for our project. Just like backing up your files on an external hard drive or in the cloud, we can Version control backup our code and manage our code. Enabling version control also makes it easy to share our scripts with collaborators and allows others to contribute to our code. 
:::

## Enable version control

![](images/git_process_placeholder.png){.shadow width="80%" fig-align="center" fig-alt="Image github workflow"}

::: notes
Here's how it works. 
:::

## Enable version control

:::{.center}

<br>

{{< fa brands github size=5x title="The GitHub octocat logo" >}}
Github

{{< fa brands gitlab size=5x title="The Gitlab tanuki logo" >}}
Gitlab

:::

::: notes
Git is the most popular version control software used with code with GitHub and GitLab probably being the two most popular centrally managed Git hosting platforms. I'll give a very brief overview of these two platforms and how you may want to use them. 
:::

## Enable version control

::: columns

::: {.column width="30%"}
 
<br>
 
{{< fa brands github size=5x title="The GitHub octocat logo" >}}


:::

::: {.column width="70%" .smaller .right}

<br>

- DOI GitHub Enterprise Cloud (DGEC) ^[[DGEC sharepoint site](https://doimspp.sharepoint.com/sites/ocio-DOI-GitHub-Enterprise/SitePages/Home.aspx)]
- USFWS Organizational Account ^[[USFWS GitHub organization](https://www.github.com/usfws)]
- USFWS Github Guidance ^[[USFWS Github sharepoint](https://doimspp.sharepoint.com/sites/fws-gis/SitePages/Using-Git-and-GitHub.aspx)]

:::

:::

::: aside
Resources:
:::

:::notes
Many of you may have experience working with GitHub and may even have a personal account; however, when we are creating code for the Service we need to ensure the code follows DOI guidelines and is preserved in a way that others can use the code after we have left the Service. 

To accomplish this, the Department of the Interior has designated GitHub as the system of record for publishing source code for collaborating and sharing between agencies and with the public. DOI has created the "DOI GitHub Enterprise Cloud" or DGEC in an effort to organize and manage control of published source code. You can visit the DGEC sharepoint page and join their Microsoft Team's site for more information on the DGEC. 

The U.S. Fish and Wildlife Service has created an organization account that is hosted under the DGEC umbrella. If you plan to share your code publicly, you'll want to host your code on the USFWS organizational account. To access the account you must create a USFWS user account using your FWS email address, complete one of the introductory DOI talent training modules for GitGub, and request access to the DGEC. For more information on this check out the sharepoint site linked below.
:::

## Version control within the Service


::: columns

::: {.column width="30%"}
 
<br>
 
{{< fa brands gitlab size=5x title="The Gitlab tanuki logo" >}}


:::

::: {.column width="70%" .smaller .right}

<br>

- USFWS user Gitlab account ^[[USFWS Gitlab](https://gitlab.fws.doi.net/)]

:::

:::

::: aside
Resources:
:::

::: notes
Alternatively, if you'd like to enable version control for a project or code you don't plan on sharing outside DOI, the service has set up a GitLab account for each of you under your MyAccount username. The Gitlab accounts are restricted to the DOI network meaning you can share code internally within DOI but cannot make your code publicly avialable through Gitlab. Now that we have version control established for our project, I will pass it over to Emma who will continue along the data workflow steps.
:::

## Import data

::: notes
-   Data do not have to be saved locally
-   Include ServCat example
:::

## Document code

::: notes
-   Commenting
-   Sections
-   Roxygen header in packages\
:::

## Tidy data

::: notes
Speaker notes go here.
:::

## Explore data/Quality control

::: notes
-   Why?
-   Options
-   Canned packages
-   Custom functions
:::

## Analyze data

::: notes
-   There is probably an R package that can do what you want
-   Advantages of scripting an analysis
-   Recommendations
-   Break into individual scripts for each step
:::

## Visualize and summarize results

::: notes
-   Tons of options, R packages
-   Static versus dynamic visualizations
-   Depends on output file type (html vs. Word)
:::

## Report results

::: notes
-   R can output to many formats
:::

## Share products

::: notes
-   You can share your products through scripts
    -   Examples
        -   Emailing
        -   Publish report or app to server (Posit Connect)
        -   Save to data repository, share link
:::

## Preserve it all

::: notes
-   You can preserve your products using scripts
:::

## Demonstrate reuse

::: notes
-   You can replicate these steps if it is all scripted
-   Documentation is important!
:::

## Where to go from here

::: notes
-   NCTC trainings
-   Resources available
-   Who to reach out to
:::
