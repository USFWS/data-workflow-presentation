---
title: "Why you should use a script-based workflow for your survey"

author: McCrea Cobb | Emma Schillerstrom | Jonah Withers
format: 
  revealjs:
    theme: scss/custom-dark.scss
    code-overflow: wrap
    logo: images/FWS-logo.png
    footer: "Alaska Data Week"
    chalkboard:
      theme: whiteboard
      boardmarker-width: 5
      buttons: false
    embed-resources: false
    code-block-height: 500px
filters:
  - output-line-highlight.lua
editor: source
highlight-style: a11y
---

```{r setup}
#| echo: false

knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      fig.retina = 3, 
                      fig.align = "center")
```

::: columns
::: {.column width="50%"}
Left column
:::

::: {.column width="50%"}
Right column
:::
:::

## Blank Slide

Content

::: notes
:::

## The Tetlin Witch Project {.smaller}

### Species Background - [*North American Witch*]{.champagne}

::: columns
::: {.column width="50%"}
#### [History:]{.extrabold .champagne}

{{< fa up-long >}} Early modern period witch trials shifted range northward

#### [Habitat:]{.extrabold .champagne}

{{< fa tree >}} Boreal forest

#### [Threats:]{.extrabold .champagne}

{{< fa broom >}} Deforestation eliminates ability to craft broomsticks

{{< fa ghost >}} Competition for resources with ghosts
:::

::: {.column width="50%"}
![Witch flying over Tetlin NWR in early fall](images/witch_tetlin.png){fig-align="left" width="4.4in"}
:::
:::

::: notes
none
:::

## The Tetlin Witch Project {.smaller}

### Survey Design - [*an Occurrence Survey*]{.champagne}

::: columns
::: {.column width="50%"}
\[Insert Map\]
:::

::: {.column width="50%"}
-   Conducted annually 20 years
-   Completed over 8 days in August
-   Each day, data is collected at 100 sites on the refuge
-   Goal:
    -   understand distribution of witches on Tetlin NWR
    -   learn about the factors that influence it
:::
:::

::: notes
lots of biotechs - obviously national funding for witch conservation
:::

## The Tetlin Witch Project {.smaller}

### Survey Design - [*an Occupancy Survey*]{.champagne}

::: columns
::: {.column width="50%"}
\[Insert Map\]
:::

::: {.column width="50%"}
-   Conducted annually 20 years
-   Completed over 8 days in August
-   Each day, data is collected at 100 sites on the refuge
-   Goal:
    -   understand [distribution of witches]{.champagne} on Tetlin NWR
    -   learn about the factors that influence it
:::
:::

::: notes
none
:::

## The Tetlin Witch Project {.smaller}

### Survey Design - [*an Occupancy Survey*]{.champagne}

::: columns
::: {.column width="50%"}
\[Insert Map\]
:::

::: {.column width="50%"}
-   Conducted annually 20 years
-   Completed over 8 days in August
-   Each day, data is collected at 100 sites on the refuge
-   Goal:
    -   understand [distribution of witches]{.champagne} on Tetlin NWR
    -   learn about the [factors that influence it]{.champagne}
:::
:::

::: notes
none
:::

## Photos from the Field

{{< fa camera >}} Coming soon to your Wild Weekly...

::: columns
::: {.column width="50%"}
![](images/witch_hiding.png){fig-align="center"}
:::

::: {.column width="50%"}
![](images/witch_moose.png){fig-align="center"}
:::
:::

::: notes
none
:::

## Where We Are

::: columns
::: {.column width="50%"}
**Completed:**

-   Data collection ✓

-   Data entry ✓

**Next:**

-   Data clean-up ☐
:::

::: {.column width="50%"}
::: {layout="[[-1], [1], [-1]]"}
![Excel workbook with all years of data](images/workbook_sc.png){fig-align="center"}
:::
:::
:::

::: notes
*No automated data processing workflow exists for this dataset, so let's create one!*
:::

## Why Clean Up Data in R? {.incremental .smaller}

::: columns
::: {.column width="50%"}
#### [Limitations of]{.cursive} {.center}

#### [Manual Workflow]{.cursive .extrabold .yellow} {.center}

::: incremental
-   Messy

-   Prone to human error

-   Difficult to correct or reverse changes

-   Inefficient

-   Not reusable
:::
:::

::: {.column width="50%"}
#### [Advantages of]{.cursive} {.center}

#### [Scripted Workflow]{.cursive .extrabold .yellow} {.center}

::: incremental
-   Can iterate through large datasets

-   Can manipulate data without overwriting it

-   Can easily undo or reapply changes

-   Changes are documented

-   Avoids repeating steps

-   Allows for automated workflows/pipelines

-   Eliminates future QC workload
:::
:::
:::

::: notes
-   efficient, reusable, not prone to error, and on top of that...

-   working outside of original file

-   can apply change to hundreds of thousands of rows of data in a single step - without ever even having to scroll through the rows

-   great for apps or lab data that comes in a set, untidy format

-   Requires medium initial effort for a future of no effort

-   can apply changes to all rows, all sheets, all files at once
:::

## Set Up New Script File

-   To use external functions, we need to load their packages
    -   First, install packages

        ```{r}
        #| echo: true
        #| eval: false
        install.packages("tidyverse","readxl","janitor")
        ```

    -   Next, use `library()` to load

        -   Sits at top of script and updated as you go

        ```{r}
        #| echo: true
        #Load packages
        library(tidyverse)
        library(readxl)
        library(janitor)
        ```

::: notes
sits at top of script
:::

## Set Up New Script File

This is called a [*comment*]{.yellow}.

```{r}
#| echo: true
#| eval: false
#| code-line-numbers: "1"
#Load packages
library(tidyverse)
library(readxl)
library(janitor)
```

::: callout-tip
## Best Practice

Document every line to few lines of code with a comment.
:::

A comment precedes [machine-readable language]{.yellow} and supplies a translation to [human-readable language]{.yellow}.

::: center
[Human Readability]{.orange .extrabold} {{< fa handshake >}} [Reusability]{.orange .extrabold}
:::

::: notes
none
:::

## Set Up New Script File

Basic documentation is as easy as a "`#`" (shortcut: `CTRL-SHIFT-C`)

```{r}
#| echo: true
#| eval: false
# Any code or text that is commented will not be executed,
but if it is not commented, R will treat it as code,
so this will return an ERROR.
```

::: callout-tip
## Tip!

The `#` can also be used to temporarily disable lines of code for testing purposes.
:::

::: notes
It serves as a reminder for what your code accomplishes and why it was included.

Translating coding language to human language is critical for yourself AND others.
:::

## Import Data

Can read data from a...

1.  [Locally stored file]{.orange} (*.csv, .xlsx, etc.*)

::: small
*Example:*
:::

```{r}
#| echo: true
#| eval: false
my_data <- read.csv("path/to/filename.csv")
```

2.  [Online source]{.orange} (*download link, API, ServCat, etc.*)

::: small
*Example:*
:::

```{r}
#| echo: true
#| eval: false
my_data <- read.csv(download.file("www.url.com","save_as_filename.csv"))
```

::: notes
Built-in and installable functions exist for reading data from various sources Like a link to the online authoritative source for a dataset
:::

## Import Data

-   For our data:
    -   Use `read_excel()` to import data from a specific sheet in a workbook
    -   Set new "dataframe" as a [*variable*]{.yellow}

```{r}
#| echo: true
#Import raw data
witch_data <- read_excel("data/witch_survey.xlsx", sheet = "2024")
```

::: callout-note
## Note

Variables are the basis of reusability!
:::

::: notes
Variables allow us to easily reference complex expressions and enable reusability. If I write all my code referencing the "witch_data" variable and I decide I want to change the input file, I can easily do that in this one line and not have to tamper with any of the rest of my code.
:::

## Tidy Data

::: {layout="[[-1], [1], [-1]]"}
![](images/tidy_graphic.png){fig-align="center"}
:::

::: notes
none
:::

## Tidy Data

Let's take a peak at what we are working with:

```{r}
#| echo: false
#| eval: true
library(gt)
gt(head(witch_data, 6)) %>% opt_stylize(style = 6, color = 'cyan')
#%>% tab_caption("First 6 lines of dataset")
```

::: center
[*First 6 lines of dataset*]{.smaller}
:::

::: callout-caution
## Why is our data not tidy?

**Answer:** Some column names are values NOT variables
:::

::: notes
-   This means we need to restructure our data
:::

## Tidy Data

Let's take a closer look at our data using some R built-in functions

-   First, with `str()`

    ```{r}
    #| echo: true
    #| eval: true
    #| class-output: highlight
    #| output-line-numbers: "|5|9|11"
    #Glimpse of dataset,including datatypes of columns
    str(witch_data)
    ```

::: notes
-   Restructing our data requires a little bit more knowledge on the state of the data because we are going to have to combine columns, and columns of different types cannot be combined
:::

## Tidy Data {.scrollable chalkboard-buttons="true"}

-   Then, with `summary()`

    ```{r}
    #| echo: true
    #| eval: true
    #| class-output: highlight
    #| output-line-numbers: "|3|10|17|14"
    #Summary of dataset, including summary stats
    summary(witch_data)
    ```

::: notes
none
:::

## Tidy Data

Let's use `tidyverse` to tidy our data in one line by "pivoting"

```{r}
#| echo: true
#Tidy data structure
tidy_data <- pivot_longer(witch_data, cols = starts_with("08"), names_to = "Date", values_to = "Presence", names_transform = list(Date = mdy), values_transform = list(Presence = as.character))
```

::: small
Updated dataframe:
:::

::: smaller
*Row Count: 800*
:::

::: {style="height:290px; overflow-y: scroll"}
```{r}
#| echo: false
gt(tidy_data) %>% opt_stylize(style = 6, color = 'cyan')
```
:::

::: notes
none
:::

## Tidy Data {.scrollable}

::: small
-   In tidy data, column names are variables, so they should be structured as such

-   Common variable naming conventions:

    -   `camelCase`
    -   `snake_case`

-   We can use the `janitor` package to fix all our column names in a single line of code
:::

```{r}
#| echo: true
#Clean column names
tidy_data <- clean_names(tidy_data)
```

```{r}
#| echo: false
gt(head(tidy_data,1)) %>% opt_stylize(style = 6, color = 'cyan')
```

::: {.smaller .center}
*New column headers*
:::

::: notes
none
:::

## Quality Control and Exploratory Data Analysis {.small}

Exploratory Data Analysis (EDA) = [getting to know your data before drawing conclusions]{.orange}, often through summarization and/or visualization

*Example packages:* `skimr`, `corrplot`, `summary_tools`, `DataExplorer`, `assertr`

<br>

::: {.cursive .small}
[Are there errors in my data?]{.fragment} [Are there outliers?]{.fragment} [How variable is my data?]{.fragment}

[Are my data within their expected range of values?]{.fragment}

[Are my variables correlated?]{.fragment} [Do my variables follow their expected distributions?]{.fragment}

[What hypotheses can I generate?]{.fragment} [Are the assumptions for my analyses met?]{.fragment}
:::

::: notes
none
:::

##  {.small}

#### Frank and Stein the Wildlife Biologists

[*Roles: field work, data recording*]{.small}

![](images/frank_stein_graphic.png){fig-align="left"}

#### Casper the friendly Biotech

[*Role: data entry/digitization*]{.small}

![](images/casper_graphic.png){fig-align="left"}

::: notes
none
:::

## Quality Control and Exploratory Data Analysis

Check to see why our `presence` column is non-numeric

```{r}
#| echo: true
#| warning: false
#Find non-numerics
tidy_data$presence[which(is.na(as.numeric(tidy_data$presence)))]
```

Replace all instances of `"none"` with a value of `0`

```{r}
#| echo: true
#Fix non-numerics
tidy_data$presence[which(tidy_data$presence == "none")] <- 0
tidy_data$presence <- as.integer(tidy_data$presence)
```

::: notes
and convert the column back to a numeric type
:::

## Quality Control and Exploratory Data Analysis {.scrollable}

-   Explore our data again

```{r}
#| echo: true
#| eval: true
#Check data
str(tidy_data)
summary(tidy_data$presence)
hist(tidy_data$presence)
```

-   Let's correct all misread numbers

```{r}
#| echo: true
#Fix typos
tidy_data$presence[which(tidy_data$presence == 6)] <- 0
tidy_data$presence[which((tidy_data$presence == 2) | (tidy_data$presence == 7))] <- 1
```

::: notes
none
:::

## Quality Control and Exploratory Data Analysis {.small}

::: columns
::: {.column width="50%"}
-   Use `DataExplorer` to generate a comprehensive report
    -   Includes summary statistics, distributions, missing data, PCA, qq plots, and more

```{r}
#| echo: true
#| eval: false
create_report(tidy_data, output_format = pdf(), output_file = "report.pdf")
```

-   Endless EDA options!

```{r}
#| eval: true
library(DataExplorer)
```
:::

::: {.column width="50%"}
{{< pdf report.pdf width=100% height=500 >}}
:::
:::

::: notes
none
:::

## Preserve Data Assets {.small .scrollable}

[*Should occur several times during data management lifecycle!*]{.champagne}

To Do List {{< fa pencil >}}:

1.  Write machine-readable metadata, including a data dictionary
2.  Preserve raw data and script file (if reusing) in ServCat

::: callout-tip
## Tip

Contact your program's data manager for program-specific preservation guidelines.

-   Fisheries and Ecological Services - *Jonah Withers*

-   Migratory Birds Management - *Tammy Patterson*

-   National Wildlife Refuge Program - *Caylen Cummins*

-   Science Applications - *Hilmar Maier*
:::

::: notes
-   Write machine-readable metadata for the tidied data file, including a data dictionary defining each of the variables (columns) and their domains
-   Preserve data, script files, and reports
    -   In ServCat, create separate reference for each and link all to the project reference corresponding to our witch survey
:::

## Preserve Data Assets

-   A step further: tidy, compile, and preserve all 20 years of data
    -   All past year sheets are in the same untidy format
    -   Frank used "none" instead of "0" since the survey first began
-   Wrapping script into a function makes this even more streamlined (*optional*)

::: notes
-   Would be a nightmare to manually fix, but we can reuse the script we just created
:::

## Preserve Data Assets

::: panel-tabset
### [Basic: Script]{.smaller}

Can reuse and rerun script for other years simply by replacing sheet name in "Import Data" step

```{r}
#| echo: true
#| eval: false
#| code-block-height: 200px

#Load packages
library(tidyverse)
library(readxl)
library(janitor)

#Import raw data
witch_data <- read_excel("witch_survey.xlsx", sheet = 1)

#Tidy data structure
tidy_data <- pivot_longer(witch_data, cols = starts_with("08"), names_to = "Date", values_to = "Presence", names_transform = list(Date = mdy), values_transform = list(Presence = as.character))

#Clean column names
tidy_data <- clean_names(tidy_data)

#Fix non-numerics
tidy_data$presence[which(tidy_data$presence == "none")] <- 0
tidy_data$presence <- as.integer(tidy_data$presence)

#Fix typos
tidy_data$presence[which(tidy_data$presence == 6)] <- 0
tidy_data$presence[which((tidy_data$presence == 2) | (tidy_data$presence == 7))] <- 1

#Export tidy csv
write.csv(tidy_data, file = "tidy_witch_data.csv", row.names = FALSE)
```

### [Advanced: Function]{.smaller}

OR wrap script into function

```{r}
#| echo: true
#| eval: false
#| code-block-height: 200px

clean_witch_data <- function(year){
  #Import raw data
  observation_data <- read_excel("witch_survey.xlsx", sheet = paste0(year))
  site_data <- read_excel("witch_survey.xlsx", sheet = "Site Info")
  
  #Relate tables: observation data and site data
  witch_data <- left_join(observation_data, site_data)
  
  #Tidy data structure
  tidy_data <- pivot_longer(witch_data, cols = starts_with("08"), names_to = "Date", values_to = "Presence", names_transform = list(Date = mdy), values_transform = list(Presence = as.character))
  
  #Clean column names
  tidy_data <- clean_names(tidy_data)
  
  #Fix non-numerics
  tidy_data$presence[which(tidy_data$presence == "none")] <- 0
  tidy_data$presence <- as.integer(tidy_data$presence)
  
  #Fix typos
  tidy_data$presence[which(tidy_data$presence == 6)] <- 0
  tidy_data$presence[which((tidy_data$presence == 2) | (tidy_data$presence == 7))] <- 1
  
  #Return tidy dataframe
  return(tidy_data)
}
```

### [Advanced: Documented Function]{.smaller}

Make sure to add proper documentation!

```{r}
#| echo: true
#| eval: false
#| code-block-height: 200px

#' Tidy and QC witch data
#'
#' @param year The year (YYYY) representing the worksheet name to pull data from
#'
#' @return A tidied dataframe
#' @export
#'
#' @examples
#' clean_witch_data(year = 2024)
clean_witch_data <- function(year){
  #Import raw data
  observation_data <- read_excel("witch_survey.xlsx", sheet = paste0(year))
  site_data <- read_excel("witch_survey.xlsx", sheet = "Site Info")
  
  #Relate tables: observation data and site data
  witch_data <- left_join(observation_data, site_data)
  
  #Tidy data structure
  tidy_data <- pivot_longer(witch_data, cols = starts_with("08"), names_to = "Date", values_to = "Presence", names_transform = list(Date = mdy), values_transform = list(Presence = as.character))
  
  #Clean column names
  tidy_data <- clean_names(tidy_data)
  
  #Fix non-numerics
  tidy_data$presence[which(tidy_data$presence == "none")] <- 0
  tidy_data$presence <- as.integer(tidy_data$presence)
  
  #Fix typos
  tidy_data$presence[which(tidy_data$presence == 6)] <- 0
  tidy_data$presence[which((tidy_data$presence == 2) | (tidy_data$presence == 7))] <- 1
  
  #Return tidy dataframe
  return(tidy_data)
}
```
:::

::: notes
none
:::

## Preserve Data Assets

Can run our function for each year and compile data as follows:

```{r}
#| echo: true
#| eval: false

# Clean and compile all years of data

#Tidy first year of data
compiled_data <- clean_witch_data(2004)

#Iterate through each year, tidy data for that year, and join with rest of tidy data
for(val in 2005:2024){
  compiled_data <- full_join(compiled_data, clean_witch_data(year = val))
}

#Export
write.csv(compiled_data, file = "compiled_witch_data.csv", row.names = FALSE)
```

::: {style="height:190px; overflow-y: scroll"}
```{r}
#| eval: TRUE
gt(tidy_data) %>% opt_stylize(style = 6)
```
:::

::: notes
none
:::

## Preserve Data Assets

Tidy, compiled data ready to be preserved and shared

::: smaller
::: {style="height:500px; overflow-y: scroll"}
```{r}
#| eval: TRUE
#gt(tidy_data) %>% opt_stylize(style = 6, color = 'white')
library(DT)
DT::datatable(tidy_data, options = list(
  columnDefs = list(
      list(targets = "_all", className = "custom-header")
    )
  )
)
```
:::
:::

::: notes
none
:::

## Preserve Data Assets

Avoid any input files through automated pulls from ServCat!

```{r}
#| eval: false
#| echo: true
library(httr)

#Request
url <- "https://ecos.fws.gov/ServCatServices/servcat-secure/v4/rest/DownloadFile/255753"
response <- GET(url = url, config = authenticate(":",":","ntlm"), add_headers("Content-Type" = "application/json"), verbose())

#Extract file name from response header
header <- headers(response)
filename <- sub('.*filename="([^"]+)".*',"\\1", header$`content-disposition`)

#Save file
writeBin(content(response,"raw"),filename)

#
new_data <- clean_witch_data("witch_data.xlsx", 2025)
old_data <- read.csv("all_witch_data.csv")
combine_data(old_data, new_data)

analyze_witch_data(new_data)
```

::: notes
none
:::

## Reuse

Like magic {{\< fa wand-sparkles \>}

-   Can now repeat all tidying, analyses, and visualizations next year with just the click of a button!
-   Medium to high initial investment for dramatically reduced future workload

::: notes
none
:::

## Reuse: Example

Fast forward to 2025 {{< fa forward-fast >}}

```{r}
#| eval: false
#| echo: true
library(httr)

#Request
url <- "https://ecos.fws.gov/ServCatServices/servcat-secure/v4/rest/DownloadFile/255753"
response <- GET(url = url, config = authenticate(":",":","ntlm"), add_headers("Content-Type" = "application/json"), verbose())

#Extract file name from response header
header <- headers(response)
filename <- sub('.*filename="([^"]+)".*',"\\1", header$`content-disposition`)

#Save file
writeBin(content(response,"raw"),filename)

#
new_data <- clean_witch_data("witch_data.xlsx", 2025)
old_data <- read.csv("all_witch_data.csv")
combine_data(old_data, new_data)

analyze_witch_data(new_data)

#Maybe code to reupload to ServCat??
```

::: notes
none
:::

## Resources

-   Available in the README.md
-   If you are in Refuges and interested in developing an automated workflow for your survey but need some assistance, submit an I&M Tech Request

::: notes
none
:::

## Import Data

-   Extra step: merge environmental site data and occurrence data

```{r}
#| echo: true
#Relate tables: observation data and site data; reorder joined columns
# witch_data <- left_join(observation_data, site_data) %>% relocate(c(12,13), .after = 3)
```

::: callout-note
## Suggestion

You can often replace complex MS Access databases with R scripts. Using R to relate tables also ensures your files are all in an open format.
:::

::: notes
none
:::
